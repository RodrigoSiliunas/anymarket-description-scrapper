"""
==========================================================================
 ‚û† Google Agent - Gemini API Integration
 ‚û† Description: AI-powered field description enhancement using Google Gemini
 ‚û† Author: Siliunas
 ‚û† System: Documentation Enhancement
==========================================================================
"""

import json
import os
import time
import concurrent.futures
from pathlib import Path
from typing import Dict, List, Any, Optional
import google.generativeai as genai
from app.configurations.configurations import AI_STUDIO_API_KEY, ROOT_PATH


class GoogleAgent:
    """
    Google Gemini AI Agent for enhancing field descriptions.
    
    Uses Google's Gemini AI to improve field descriptions with professional,
    detailed explanations following enterprise documentation standards.
    """
    
    def __init__(self, fast_mode: bool = False):
        """Initialize the Google Agent with API configuration."""
        self.api_key = AI_STUDIO_API_KEY
        self.input_dir = Path(ROOT_PATH) / "app" / "assets" / "out"
        self.output_dir = Path(ROOT_PATH) / "app" / "assets" / "agent"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.fast_mode = fast_mode
        
        # Configure Gemini
        if not self.api_key:
            raise ValueError("AI_STUDIO_API_KEY not found in environment variables")
        
        genai.configure(api_key=self.api_key)
        
        # Configure model with generation settings for better quality
        generation_config = genai.types.GenerationConfig(
            temperature=0.3,  # Lower temperature for more consistent output
            top_p=0.8,
            top_k=40,
            max_output_tokens=200,  # Limit output length
        )
        
        # Configure safety and timeout settings to reduce retries
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        
        self.model = genai.GenerativeModel(
            'gemini-2.5-flash-lite',
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        
        # Rate limiting - otimizado para conta paga
        if fast_mode:
            self.request_delay = 0.1    # 100ms entre requests
            self.batch_delay = 0.5      # 0.5 segundo entre batches
            self.batch_size = 20        # 20 campos por batch
            print("üöÄ Modo r√°pido ativado!")
            print("‚è∞ Timeout: 20 segundos por requisi√ß√£o com retry autom√°tico")
        else:
            self.request_delay = 0.5    # 500ms entre requests
            self.batch_delay = 2.0      # 2 segundos entre batches
            self.batch_size = 10        # 10 campos por batch
        
        # Request tracking para debug
        self.request_count = 0
        self.timeout_count = 0
        self.fallback_count = 0
        self.start_time = time.time()
        
    def _create_description_prompt(self, field_data: Dict[str, Any], table_name: str) -> str:
        """
        Create a detailed prompt for Gemini to generate better field descriptions.
        
        Args:
            field_data: Field information including name, type, original description
            table_name: Name of the table/endpoint
            
        Returns:
            Formatted prompt string
        """
        examples = """
EXCELLENT EXAMPLES of professional API field descriptions:
‚úÖ "This field represents the unique category ID generated by Anymarket, returned as an integer (>= 0) from the categories endpoint."
‚úÖ "This column stores the category's hierarchical path structure (directories/subdirectories), typically up to 1000 characters from the Anymarket categories endpoint."
‚úÖ "This field contains the partner-specific category identifier, provided as a string value through the Anymarket categories endpoint."
‚úÖ "This column indicates the pricing multiplication factor applied to this category, represented as an integer value from the Anymarket categories endpoint."
‚úÖ "This field represents whether fulfillment services are enabled for this order, returned as a boolean (true/false) from the Anymarket orders endpoint."
"""
        
        original_desc = field_data.get('description', 'No description')
        field_name = field_data['name']
        field_type = field_data['field_type']
        
        prompt = f"""Write a short API field description:

Field: {field_name} ({field_type})
API: Anymarket {table_name}

Start with "This field" and mention "Anymarket {table_name} endpoint". Max 100 chars.

Description:"""
        
        return prompt
    
    def _enhance_field_description(self, field_data: Dict[str, Any], table_name: str) -> str:
        """
        Use Gemini to enhance a single field description.
        
        Args:
            field_data: Field information
            table_name: Table name for context
            
        Returns:
            Enhanced description
        """
        try:
            # Incrementar contador de requisi√ß√µes
            self.request_count += 1
            print(f"    üì° Request #{self.request_count} for {field_data['name']}")
            
            prompt = self._create_description_prompt(field_data, table_name)
            
            # Generate content with Gemini - timeout com retry
            def generate_with_timeout():
                try:
                    return self.model.generate_content(prompt)
                except Exception as e:
                    raise TimeoutError(f"Gemini API error: {str(e)}")
            
            # Timeout de 20 segundos com retry
            max_attempts = 2
            for attempt in range(max_attempts):
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(generate_with_timeout)
                    try:
                        response = future.result(timeout=20.0)  # 20s timeout
                        break  # Sucesso, sair do loop
                    except (concurrent.futures.TimeoutError, Exception) as timeout_error:
                        attempt_text = f"(tentativa {attempt + 1}/{max_attempts})"
                        if attempt < max_attempts - 1:
                            print(f"    ‚è∞ Timeout para {field_data['name']} {attempt_text}, tentando novamente...")
                            future.cancel()
                            time.sleep(1)  # Pequeno delay antes do retry
                            continue
                        else:
                            print(f"    ‚ùå Timeout final para {field_data['name']} {attempt_text}, usando fallback...")
                            self.timeout_count += 1
                            future.cancel()
                            return self._generate_fallback_description(field_data, table_name)
            
            # Clean and validate response
            enhanced_description = response.text.strip()
            original_description = field_data.get('description', '')
            
            # Quality checks
            if not enhanced_description:
                print(f"    ‚ö†Ô∏è Empty response for {field_data['name']}")
                return original_description or f"Field {field_data['name']} from Anymarket {table_name} endpoint."
            
            # Check if response is too long
            if len(enhanced_description) > 250:
                enhanced_description = enhanced_description[:247] + "..."
                
            # Check if it's actually enhanced (not just copy)
            if enhanced_description == original_description:
                print(f"    ‚ö†Ô∏è No enhancement for {field_data['name']}, generating fallback...")
                self.fallback_count += 1
                enhanced_description = self._generate_fallback_description(field_data, table_name)
            
            # Check if it starts correctly
            valid_starts = ["This field", "This column", "This property", "This attribute"]
            if not any(enhanced_description.startswith(start) for start in valid_starts):
                print(f"    ‚ö†Ô∏è Invalid format for {field_data['name']}, generating fallback...")
                self.fallback_count += 1
                enhanced_description = self._generate_fallback_description(field_data, table_name)
            
            return enhanced_description
            
        except Exception as e:
            print(f"    ‚ùå Error enhancing {field_data['name']}: {str(e)}")
            return self._generate_fallback_description(field_data, table_name)
    
    def _generate_fallback_description(self, field_data: Dict[str, Any], table_name: str) -> str:
        """
        Generate a fallback description when AI enhancement fails.
        
        Args:
            field_data: Field information
            table_name: Table name for context
            
        Returns:
            Fallback description
        """
        field_name = field_data['name']
        field_type = field_data['field_type']
        
        # Type-specific templates
        if 'id' in field_name.lower():
            return f"This field represents the unique {field_name.replace('_', ' ')} identifier from the Anymarket {table_name} endpoint, returned as {field_type}."
        
        elif field_type == 'boolean':
            return f"This field indicates the {field_name.replace('_', ' ')} status, returned as a boolean (true/false) from the Anymarket {table_name} endpoint."
        
        elif field_type == 'integer' or 'int' in field_type:
            return f"This field contains the {field_name.replace('_', ' ')} value, returned as an integer from the Anymarket {table_name} endpoint."
        
        elif field_type == 'string':
            return f"This field stores the {field_name.replace('_', ' ')} information as a string value from the Anymarket {table_name} endpoint."
        
        elif 'array' in field_type:
            return f"This field contains an array of {field_name.replace('_', ' ')} objects from the Anymarket {table_name} endpoint."
        
        elif field_type == 'object':
            return f"This field represents a {field_name.replace('_', ' ')} object structure from the Anymarket {table_name} endpoint."
        
        else:
            return f"This field contains {field_name.replace('_', ' ')} data from the Anymarket {table_name} endpoint, returned as {field_type}."
    
    def _process_file(self, input_file: Path) -> Dict[str, Any]:
        """
        Process a single JSON file and enhance all field descriptions.
        
        Args:
            input_file: Path to input JSON file
            
        Returns:
            Enhanced data structure
        """
        print(f"üìä Processing: {input_file.name}")
        
        # Load original data
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Prepare enhanced data structure
        enhanced_data = {
            "table_name": data.get("table_name", input_file.stem),
            "extraction_date": data.get("extraction_date"),
            "processing_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "ai_enhancement_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_fields": len(data.get("fields", [])),
            "fields": [],
            "enhancement_summary": {
                "total_fields_enhanced": 0,
                "failed_enhancements": 0,
                "model_used": "gemini-2.5-flash-lite"
            }
        }
        
        # Process fields individually to avoid double API calls
        table_name = data.get("table_name", input_file.stem)
        enhanced_count = 0
        failed_count = 0
        total_fields = len(data.get("fields", []))
        
        print(f"  üîÑ Processing {total_fields} fields individually (20s timeout + retry)")
        print(f"  ‚è∞ Automatic retry on timeout, fallback on final failure")
        
        # Process each field individually
        for i, field in enumerate(data.get("fields", []), 1):
            print(f"  ü§ñ Enhancing field {i}/{total_fields}: {field['name']}", end="")
            
            # Create enhanced field
            enhanced_field = field.copy()
            
            # Get enhanced description - SINGLE CALL
            original_desc = field.get('description', '')
            start_time = time.time()
            enhanced_desc = self._enhance_field_description(field, table_name)
            elapsed = time.time() - start_time
            
            print(f" ({elapsed:.1f}s)")
            
            # Update field with enhanced description
            enhanced_field['enhanced_description'] = enhanced_desc
            enhanced_field['original_description'] = original_desc
            
            # Track enhancement success
            is_enhanced = (
                enhanced_desc != original_desc and 
                enhanced_desc.startswith(('This field', 'This column', 'This property', 'This attribute')) and
                'Anymarket' in enhanced_desc and
                len(enhanced_desc) > 50
            )
            
            if is_enhanced:
                enhanced_count += 1
                print(f"    ‚úÖ Enhanced successfully")
            else:
                failed_count += 1
                print(f"    ‚ö†Ô∏è Enhancement failed or minimal improvement")
            
            enhanced_data["fields"].append(enhanced_field)
            
            # Rate limiting
            if i < total_fields:  # Don't delay after last field
                time.sleep(self.request_delay)
        
        # Update summary
        enhanced_data["enhancement_summary"]["total_fields_enhanced"] = enhanced_count
        enhanced_data["enhancement_summary"]["failed_enhancements"] = failed_count
        
        return enhanced_data
    
    def process_all_files(self) -> None:
        """
        Process all JSON files in the input directory and save enhanced versions.
        Includes comprehensive progress tracking and error handling.
        """
        if not self.input_dir.exists():
            print(f"‚ùå Input directory not found: {self.input_dir}")
            return
        
        json_files = list(self.input_dir.glob("*.json"))
        
        if not json_files:
            print(f"‚ùå No JSON files found in: {self.input_dir}")
            return
        
        print(f"üöÄ Starting AI enhancement for {len(json_files)} files")
        print(f"üìÇ Input: {self.input_dir}")
        print(f"üìÇ Output: {self.output_dir}")
        print(f"‚ö° Mode: {'Fast' if self.fast_mode else 'Normal'}")
        print(f"‚è∞ Timeout: 20s per request with automatic retry")
        print("=" * 60)
        
        # Statistics tracking
        total_enhanced = 0
        total_failed = 0
        total_retries = 0
        start_time = time.time()
        processed_files = []
        failed_files = []
        
        for file_index, json_file in enumerate(json_files, 1):
            try:
                print(f"\nüìÅ [{file_index}/{len(json_files)}] Processing {json_file.name}...")
                file_start = time.time()
                
                # Reset file-specific counters
                initial_request_count = self.request_count
                initial_timeout_count = self.timeout_count
                
                # Process file
                enhanced_data = self._process_file(json_file)
                
                file_elapsed = time.time() - file_start
                
                # Save enhanced file
                output_file = self.output_dir / f"{json_file.stem}_enhanced.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(enhanced_data, f, indent=2, ensure_ascii=False)
                
                # Update totals
                summary = enhanced_data["enhancement_summary"]
                file_enhanced = summary["total_fields_enhanced"]
                file_failed = summary["failed_enhancements"]
                file_requests = self.request_count - initial_request_count
                file_timeouts = self.timeout_count - initial_timeout_count
                
                total_enhanced += file_enhanced
                total_failed += file_failed
                
                # Calculate statistics
                total_elapsed = time.time() - start_time
                avg_time_per_file = total_elapsed / file_index
                remaining_files = len(json_files) - file_index
                eta_seconds = remaining_files * avg_time_per_file
                eta_minutes = eta_seconds / 60
                
                # File success rate
                file_success_rate = (file_enhanced / (file_enhanced + file_failed)) * 100 if (file_enhanced + file_failed) > 0 else 0
                
                print(f"‚úÖ Saved: {output_file.name}")
                print(f"   üìä Enhanced: {file_enhanced}, Failed: {file_failed} (Success: {file_success_rate:.1f}%)")
                print(f"   üì° Requests: {file_requests}, Timeouts: {file_timeouts}")
                print(f"   ‚è±Ô∏è File time: {file_elapsed:.1f}s | ETA: {eta_minutes:.1f} min")
                
                processed_files.append({
                    'file': json_file.name,
                    'enhanced': file_enhanced,
                    'failed': file_failed,
                    'requests': file_requests,
                    'timeouts': file_timeouts,
                    'time': file_elapsed
                })
                
            except KeyboardInterrupt:
                print(f"\n‚ö†Ô∏è Process interrupted by user")
                break
            except Exception as e:
                print(f"‚ùå Error processing {json_file.name}: {str(e)}")
                failed_files.append({'file': json_file.name, 'error': str(e)})
                continue
        
        # Final statistics
        total_elapsed = time.time() - start_time
        overall_success_rate = (total_enhanced / (total_enhanced + total_failed)) * 100 if (total_enhanced + total_failed) > 0 else 0
        timeout_rate = (self.timeout_count / self.request_count) * 100 if self.request_count > 0 else 0
        
        print("\n" + "=" * 60)
        print(f"üéâ Enhancement completed!")
        print(f"üìä FINAL STATISTICS:")
        print(f"   üìÅ Files processed: {len(processed_files)}/{len(json_files)}")
        print(f"   ‚úÖ Fields enhanced: {total_enhanced}")
        print(f"   ‚ùå Fields failed: {total_failed}")
        print(f"   üìà Success rate: {overall_success_rate:.1f}%")
        print(f"   üì° Total requests: {self.request_count}")
        print(f"   ‚è∞ Timeouts: {self.timeout_count} ({timeout_rate:.1f}%)")
        print(f"   üîÑ Fallbacks: {self.fallback_count}")
        print(f"   ‚è±Ô∏è Total time: {total_elapsed:.1f}s")
        print(f"   üìÅ Enhanced files saved in: {self.output_dir}")
        
        # Performance analysis
        if timeout_rate > 30:
            print(f"\n‚ö†Ô∏è High timeout rate ({timeout_rate:.1f}%)!")
            print(f"   This may explain extra requests in Google Console.")
            print(f"   Consider reducing request complexity or increasing timeout.")
        
        if self.fallback_count > 0:
            fallback_rate = (self.fallback_count / self.request_count) * 100
            print(f"\nüìä Fallback usage: {fallback_rate:.1f}% of requests")
        
        # Failed files summary
        if failed_files:
            print(f"\n‚ùå Failed files ({len(failed_files)}):")
            for failed in failed_files:
                print(f"   ‚Ä¢ {failed['file']}: {failed['error']}")
        
        # Performance recommendations
        if total_elapsed > 0:
            avg_time_per_field = total_elapsed / max(self.request_count, 1)
            print(f"\nüìä Performance: {avg_time_per_field:.2f}s per field average")
            
            if avg_time_per_field > 10:
                print("üí° Recommendation: Consider enabling fast_mode for better performance")
            elif timeout_rate > 20:
                print("üí° Recommendation: API may be slow, consider processing in smaller batches")
    
    def process_single_file(self, filename: str) -> None:
        """
        Process a single file by name.
        
        Args:
            filename: Name of the JSON file to process
        """
        input_file = self.input_dir / filename
        
        if not input_file.exists():
            print(f"‚ùå File not found: {input_file}")
            return
        
        try:
            enhanced_data = self._process_file(input_file)
            
            output_file = self.output_dir / f"{input_file.stem}_enhanced.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(enhanced_data, f, indent=2, ensure_ascii=False)
            
            summary = enhanced_data["enhancement_summary"]
            
            # Estat√≠sticas detalhadas sobre requisi√ß√µes
            print(f"‚úÖ Enhanced {filename}")
            print(f"üìä Enhanced: {summary['total_fields_enhanced']}, Failed: {summary['failed_enhancements']}")
            print(f"üì° Total requests: {self.request_count}")
            print(f"‚è∞ Timeouts: {self.timeout_count}")
            print(f"üîÑ Fallbacks: {self.fallback_count}")
            
            if self.timeout_count > 0:
                timeout_rate = (self.timeout_count / self.request_count) * 100
                print(f"üìä Timeout rate: {timeout_rate:.1f}%")
                if timeout_rate > 30:
                    print(f"‚ö†Ô∏è High timeout rate! This explains the extra requests in Google Console.")
                    print(f"   Recommendation: Google may be retrying failed requests internally.")
            
            print(f"üíæ Saved: {output_file}")
            
        except Exception as e:
            print(f"‚ùå Error processing {filename}: {str(e)}")


def test_google_agent():
    """Test function for GoogleAgent."""
    try:
        agent = GoogleAgent(fast_mode=True)
        print("ü§ñ GoogleAgent initialized successfully!")
        print("üß™ Testing with 20s timeout + retry system...")
        agent.process_single_file("categories.json")
        
    except Exception as e:
        print(f"‚ùå Error testing GoogleAgent: {str(e)}")


if __name__ == "__main__":
    test_google_agent()
