"""
==========================================================================
 ➠ Google Agent - Gemini API Integration
 ➠ Description: AI-powered field description enhancement using Google Gemini
 ➠ Author: Siliunas
 ➠ System: Documentation Enhancement
==========================================================================
"""

import json
import os
import time
import concurrent.futures
from pathlib import Path
from typing import Dict, List, Any, Optional
import google.generativeai as genai
from app.configurations.configurations import AI_STUDIO_API_KEY, ROOT_PATH


class GoogleAgent:
    """
    Google Gemini AI Agent for enhancing field descriptions.
    
    Uses Google's Gemini AI to improve field descriptions with professional,
    detailed explanations following enterprise documentation standards.
    """
    
    def __init__(self, fast_mode: bool = False):
        """Initialize the Google Agent with API configuration."""
        self.api_key = AI_STUDIO_API_KEY
        self.input_dir = Path(ROOT_PATH) / "app" / "assets" / "out"
        self.output_dir = Path(ROOT_PATH) / "app" / "assets" / "agent"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.fast_mode = fast_mode
        
        # Configure Gemini
        if not self.api_key:
            raise ValueError("AI_STUDIO_API_KEY not found in environment variables")
        
        genai.configure(api_key=self.api_key)
        
        # Configure model with generation settings for better quality
        generation_config = genai.types.GenerationConfig(
            temperature=0.3,  # Lower temperature for more consistent output
            top_p=0.8,
            top_k=40,
            max_output_tokens=200,  # Limit output length
        )
        
        # Configure safety and timeout settings to reduce retries
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        
        self.model = genai.GenerativeModel(
            'gemini-2.5-flash-lite',
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        
        # Rate limiting - otimizado para conta paga
        if fast_mode:
            self.request_delay = 0.1    # 100ms entre requests
            self.batch_delay = 0.5      # 0.5 segundo entre batches
            self.batch_size = 20        # 20 campos por batch
            print("🚀 Modo rápido ativado!")
            print("⏰ Timeout: 20 segundos por requisição com retry automático")
        else:
            self.request_delay = 0.5    # 500ms entre requests
            self.batch_delay = 2.0      # 2 segundos entre batches
            self.batch_size = 10        # 10 campos por batch
        
        # Request tracking para debug
        self.request_count = 0
        self.timeout_count = 0
        self.fallback_count = 0
        self.start_time = time.time()
        
    def _create_description_prompt(self, field_data: Dict[str, Any], table_name: str) -> str:
        """
        Create a detailed prompt for Gemini to generate better field descriptions.
        
        Args:
            field_data: Field information including name, type, original description
            table_name: Name of the table/endpoint
            
        Returns:
            Formatted prompt string
        """
        examples = """
EXCELLENT EXAMPLES of professional API field descriptions:
✅ "This field represents the unique category ID generated by Anymarket, returned as an integer (>= 0) from the categories endpoint."
✅ "This column stores the category's hierarchical path structure (directories/subdirectories), typically up to 1000 characters from the Anymarket categories endpoint."
✅ "This field contains the partner-specific category identifier, provided as a string value through the Anymarket categories endpoint."
✅ "This column indicates the pricing multiplication factor applied to this category, represented as an integer value from the Anymarket categories endpoint."
✅ "This field represents whether fulfillment services are enabled for this order, returned as a boolean (true/false) from the Anymarket orders endpoint."
"""
        
        original_desc = field_data.get('description', 'No description')
        field_name = field_data['name']
        field_type = field_data['field_type']
        
        prompt = f"""Write a short API field description:

Field: {field_name} ({field_type})
API: Anymarket {table_name}

Start with "This field" and mention "Anymarket {table_name} endpoint". Max 100 chars.

Description:"""
        
        return prompt
    
    def _enhance_field_description(self, field_data: Dict[str, Any], table_name: str) -> str:
        """
        Use Gemini to enhance a single field description.
        
        Args:
            field_data: Field information
            table_name: Table name for context
            
        Returns:
            Enhanced description
        """
        try:
            # Incrementar contador de requisições
            self.request_count += 1
            print(f"    📡 Request #{self.request_count} for {field_data['name']}")
            
            prompt = self._create_description_prompt(field_data, table_name)
            
            # Generate content with Gemini - timeout com retry
            def generate_with_timeout():
                try:
                    return self.model.generate_content(prompt)
                except Exception as e:
                    raise TimeoutError(f"Gemini API error: {str(e)}")
            
            # Timeout de 20 segundos com retry
            max_attempts = 2
            for attempt in range(max_attempts):
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(generate_with_timeout)
                    try:
                        response = future.result(timeout=20.0)  # 20s timeout
                        break  # Sucesso, sair do loop
                    except (concurrent.futures.TimeoutError, Exception) as timeout_error:
                        attempt_text = f"(tentativa {attempt + 1}/{max_attempts})"
                        if attempt < max_attempts - 1:
                            print(f"    ⏰ Timeout para {field_data['name']} {attempt_text}, tentando novamente...")
                            future.cancel()
                            time.sleep(1)  # Pequeno delay antes do retry
                            continue
                        else:
                            print(f"    ❌ Timeout final para {field_data['name']} {attempt_text}, usando fallback...")
                            self.timeout_count += 1
                            future.cancel()
                            return self._generate_fallback_description(field_data, table_name)
            
            # Clean and validate response
            enhanced_description = response.text.strip()
            original_description = field_data.get('description', '')
            
            # Quality checks
            if not enhanced_description:
                print(f"    ⚠️ Empty response for {field_data['name']}")
                return original_description or f"Field {field_data['name']} from Anymarket {table_name} endpoint."
            
            # Check if response is too long
            if len(enhanced_description) > 250:
                enhanced_description = enhanced_description[:247] + "..."
                
            # Check if it's actually enhanced (not just copy)
            if enhanced_description == original_description:
                print(f"    ⚠️ No enhancement for {field_data['name']}, generating fallback...")
                self.fallback_count += 1
                enhanced_description = self._generate_fallback_description(field_data, table_name)
            
            # Check if it starts correctly
            valid_starts = ["This field", "This column", "This property", "This attribute"]
            if not any(enhanced_description.startswith(start) for start in valid_starts):
                print(f"    ⚠️ Invalid format for {field_data['name']}, generating fallback...")
                self.fallback_count += 1
                enhanced_description = self._generate_fallback_description(field_data, table_name)
            
            return enhanced_description
            
        except Exception as e:
            print(f"    ❌ Error enhancing {field_data['name']}: {str(e)}")
            return self._generate_fallback_description(field_data, table_name)
    
    def _generate_fallback_description(self, field_data: Dict[str, Any], table_name: str) -> str:
        """
        Generate a fallback description when AI enhancement fails.
        
        Args:
            field_data: Field information
            table_name: Table name for context
            
        Returns:
            Fallback description
        """
        field_name = field_data['name']
        field_type = field_data['field_type']
        
        # Type-specific templates
        if 'id' in field_name.lower():
            return f"This field represents the unique {field_name.replace('_', ' ')} identifier from the Anymarket {table_name} endpoint, returned as {field_type}."
        
        elif field_type == 'boolean':
            return f"This field indicates the {field_name.replace('_', ' ')} status, returned as a boolean (true/false) from the Anymarket {table_name} endpoint."
        
        elif field_type == 'integer' or 'int' in field_type:
            return f"This field contains the {field_name.replace('_', ' ')} value, returned as an integer from the Anymarket {table_name} endpoint."
        
        elif field_type == 'string':
            return f"This field stores the {field_name.replace('_', ' ')} information as a string value from the Anymarket {table_name} endpoint."
        
        elif 'array' in field_type:
            return f"This field contains an array of {field_name.replace('_', ' ')} objects from the Anymarket {table_name} endpoint."
        
        elif field_type == 'object':
            return f"This field represents a {field_name.replace('_', ' ')} object structure from the Anymarket {table_name} endpoint."
        
        else:
            return f"This field contains {field_name.replace('_', ' ')} data from the Anymarket {table_name} endpoint, returned as {field_type}."
    
    def _process_file(self, input_file: Path) -> Dict[str, Any]:
        """
        Process a single JSON file and enhance all field descriptions.
        
        Args:
            input_file: Path to input JSON file
            
        Returns:
            Enhanced data structure
        """
        print(f"📊 Processing: {input_file.name}")
        
        # Load original data
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Prepare enhanced data structure
        enhanced_data = {
            "table_name": data.get("table_name", input_file.stem),
            "extraction_date": data.get("extraction_date"),
            "processing_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "ai_enhancement_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_fields": len(data.get("fields", [])),
            "fields": [],
            "enhancement_summary": {
                "total_fields_enhanced": 0,
                "failed_enhancements": 0,
                "model_used": "gemini-2.5-flash-lite"
            }
        }
        
        # Process fields individually to avoid double API calls
        table_name = data.get("table_name", input_file.stem)
        enhanced_count = 0
        failed_count = 0
        total_fields = len(data.get("fields", []))
        
        print(f"  🔄 Processing {total_fields} fields individually (20s timeout + retry)")
        print(f"  ⏰ Automatic retry on timeout, fallback on final failure")
        
        # Process each field individually
        for i, field in enumerate(data.get("fields", []), 1):
            print(f"  🤖 Enhancing field {i}/{total_fields}: {field['name']}", end="")
            
            # Create enhanced field
            enhanced_field = field.copy()
            
            # Get enhanced description - SINGLE CALL
            original_desc = field.get('description', '')
            start_time = time.time()
            enhanced_desc = self._enhance_field_description(field, table_name)
            elapsed = time.time() - start_time
            
            print(f" ({elapsed:.1f}s)")
            
            # Update field with enhanced description
            enhanced_field['enhanced_description'] = enhanced_desc
            enhanced_field['original_description'] = original_desc
            
            # Track enhancement success
            is_enhanced = (
                enhanced_desc != original_desc and 
                enhanced_desc.startswith(('This field', 'This column', 'This property', 'This attribute')) and
                'Anymarket' in enhanced_desc and
                len(enhanced_desc) > 50
            )
            
            if is_enhanced:
                enhanced_count += 1
                print(f"    ✅ Enhanced successfully")
            else:
                failed_count += 1
                print(f"    ⚠️ Enhancement failed or minimal improvement")
            
            enhanced_data["fields"].append(enhanced_field)
            
            # Rate limiting
            if i < total_fields:  # Don't delay after last field
                time.sleep(self.request_delay)
        
        # Update summary
        enhanced_data["enhancement_summary"]["total_fields_enhanced"] = enhanced_count
        enhanced_data["enhancement_summary"]["failed_enhancements"] = failed_count
        
        return enhanced_data
    
    def process_all_files(self) -> None:
        """
        Process all JSON files in the input directory and save enhanced versions.
        Includes comprehensive progress tracking and error handling.
        """
        if not self.input_dir.exists():
            print(f"❌ Input directory not found: {self.input_dir}")
            return
        
        json_files = list(self.input_dir.glob("*.json"))
        
        if not json_files:
            print(f"❌ No JSON files found in: {self.input_dir}")
            return
        
        print(f"🚀 Starting AI enhancement for {len(json_files)} files")
        print(f"📂 Input: {self.input_dir}")
        print(f"📂 Output: {self.output_dir}")
        print(f"⚡ Mode: {'Fast' if self.fast_mode else 'Normal'}")
        print(f"⏰ Timeout: 20s per request with automatic retry")
        print("=" * 60)
        
        # Statistics tracking
        total_enhanced = 0
        total_failed = 0
        total_retries = 0
        start_time = time.time()
        processed_files = []
        failed_files = []
        
        for file_index, json_file in enumerate(json_files, 1):
            try:
                print(f"\n📁 [{file_index}/{len(json_files)}] Processing {json_file.name}...")
                file_start = time.time()
                
                # Reset file-specific counters
                initial_request_count = self.request_count
                initial_timeout_count = self.timeout_count
                
                # Process file
                enhanced_data = self._process_file(json_file)
                
                file_elapsed = time.time() - file_start
                
                # Save enhanced file
                output_file = self.output_dir / f"{json_file.stem}_enhanced.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(enhanced_data, f, indent=2, ensure_ascii=False)
                
                # Update totals
                summary = enhanced_data["enhancement_summary"]
                file_enhanced = summary["total_fields_enhanced"]
                file_failed = summary["failed_enhancements"]
                file_requests = self.request_count - initial_request_count
                file_timeouts = self.timeout_count - initial_timeout_count
                
                total_enhanced += file_enhanced
                total_failed += file_failed
                
                # Calculate statistics
                total_elapsed = time.time() - start_time
                avg_time_per_file = total_elapsed / file_index
                remaining_files = len(json_files) - file_index
                eta_seconds = remaining_files * avg_time_per_file
                eta_minutes = eta_seconds / 60
                
                # File success rate
                file_success_rate = (file_enhanced / (file_enhanced + file_failed)) * 100 if (file_enhanced + file_failed) > 0 else 0
                
                print(f"✅ Saved: {output_file.name}")
                print(f"   📊 Enhanced: {file_enhanced}, Failed: {file_failed} (Success: {file_success_rate:.1f}%)")
                print(f"   📡 Requests: {file_requests}, Timeouts: {file_timeouts}")
                print(f"   ⏱️ File time: {file_elapsed:.1f}s | ETA: {eta_minutes:.1f} min")
                
                processed_files.append({
                    'file': json_file.name,
                    'enhanced': file_enhanced,
                    'failed': file_failed,
                    'requests': file_requests,
                    'timeouts': file_timeouts,
                    'time': file_elapsed
                })
                
            except KeyboardInterrupt:
                print(f"\n⚠️ Process interrupted by user")
                break
            except Exception as e:
                print(f"❌ Error processing {json_file.name}: {str(e)}")
                failed_files.append({'file': json_file.name, 'error': str(e)})
                continue
        
        # Final statistics
        total_elapsed = time.time() - start_time
        overall_success_rate = (total_enhanced / (total_enhanced + total_failed)) * 100 if (total_enhanced + total_failed) > 0 else 0
        timeout_rate = (self.timeout_count / self.request_count) * 100 if self.request_count > 0 else 0
        
        print("\n" + "=" * 60)
        print(f"🎉 Enhancement completed!")
        print(f"📊 FINAL STATISTICS:")
        print(f"   📁 Files processed: {len(processed_files)}/{len(json_files)}")
        print(f"   ✅ Fields enhanced: {total_enhanced}")
        print(f"   ❌ Fields failed: {total_failed}")
        print(f"   📈 Success rate: {overall_success_rate:.1f}%")
        print(f"   📡 Total requests: {self.request_count}")
        print(f"   ⏰ Timeouts: {self.timeout_count} ({timeout_rate:.1f}%)")
        print(f"   🔄 Fallbacks: {self.fallback_count}")
        print(f"   ⏱️ Total time: {total_elapsed:.1f}s")
        print(f"   📁 Enhanced files saved in: {self.output_dir}")
        
        # Performance analysis
        if timeout_rate > 30:
            print(f"\n⚠️ High timeout rate ({timeout_rate:.1f}%)!")
            print(f"   This may explain extra requests in Google Console.")
            print(f"   Consider reducing request complexity or increasing timeout.")
        
        if self.fallback_count > 0:
            fallback_rate = (self.fallback_count / self.request_count) * 100
            print(f"\n📊 Fallback usage: {fallback_rate:.1f}% of requests")
        
        # Failed files summary
        if failed_files:
            print(f"\n❌ Failed files ({len(failed_files)}):")
            for failed in failed_files:
                print(f"   • {failed['file']}: {failed['error']}")
        
        # Performance recommendations
        if total_elapsed > 0:
            avg_time_per_field = total_elapsed / max(self.request_count, 1)
            print(f"\n📊 Performance: {avg_time_per_field:.2f}s per field average")
            
            if avg_time_per_field > 10:
                print("💡 Recommendation: Consider enabling fast_mode for better performance")
            elif timeout_rate > 20:
                print("💡 Recommendation: API may be slow, consider processing in smaller batches")
    
    def process_single_file(self, filename: str) -> None:
        """
        Process a single file by name.
        
        Args:
            filename: Name of the JSON file to process
        """
        input_file = self.input_dir / filename
        
        if not input_file.exists():
            print(f"❌ File not found: {input_file}")
            return
        
        try:
            enhanced_data = self._process_file(input_file)
            
            output_file = self.output_dir / f"{input_file.stem}_enhanced.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(enhanced_data, f, indent=2, ensure_ascii=False)
            
            summary = enhanced_data["enhancement_summary"]
            
            # Estatísticas detalhadas sobre requisições
            print(f"✅ Enhanced {filename}")
            print(f"📊 Enhanced: {summary['total_fields_enhanced']}, Failed: {summary['failed_enhancements']}")
            print(f"📡 Total requests: {self.request_count}")
            print(f"⏰ Timeouts: {self.timeout_count}")
            print(f"🔄 Fallbacks: {self.fallback_count}")
            
            if self.timeout_count > 0:
                timeout_rate = (self.timeout_count / self.request_count) * 100
                print(f"📊 Timeout rate: {timeout_rate:.1f}%")
                if timeout_rate > 30:
                    print(f"⚠️ High timeout rate! This explains the extra requests in Google Console.")
                    print(f"   Recommendation: Google may be retrying failed requests internally.")
            
            print(f"💾 Saved: {output_file}")
            
        except Exception as e:
            print(f"❌ Error processing {filename}: {str(e)}")


def test_google_agent():
    """Test function for GoogleAgent."""
    try:
        agent = GoogleAgent(fast_mode=True)
        print("🤖 GoogleAgent initialized successfully!")
        print("🧪 Testing with 20s timeout + retry system...")
        agent.process_single_file("categories.json")
        
    except Exception as e:
        print(f"❌ Error testing GoogleAgent: {str(e)}")


if __name__ == "__main__":
    test_google_agent()
